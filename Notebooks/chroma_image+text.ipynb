{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: chromadb in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (1.4.1)\n",
      "Collecting pillow\n",
      "  Using cached pillow-12.1.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from sentence-transformers) (1.3.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.10.0-cp310-cp310-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from sentence-transformers) (2.2.6)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2026.1.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (2.12.5)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (0.50.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (35.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (3.11.6)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (14.3.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.6.3)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from build>=1.0.3->chromadb) (8.7.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.4.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb) (3.23.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.5)\n",
      "Requirement already satisfied: sympy in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\work\\learning\\gen-ai\\vectordb_venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading pymupdf-1.26.7-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 5.8/18.4 MB 35.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 14.2/18.4 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 35.2 MB/s  0:00:00\n",
      "Downloading sentence_transformers-5.2.2-py3-none-any.whl (494 kB)\n",
      "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 8.4/10.1 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 22.6 MB/s  0:00:00\n",
      "Using cached pillow-12.1.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Downloading regex-2026.1.15-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading torch-2.10.0-cp310-cp310-win_amd64.whl (113.7 MB)\n",
      "   ---------------------------------------- 0.0/113.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 4.2/113.7 MB 21.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.9/113.7 MB 21.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 12.3/113.7 MB 21.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 12.8/113.7 MB 18.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 16.3/113.7 MB 16.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 19.1/113.7 MB 15.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 24.1/113.7 MB 16.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 25.2/113.7 MB 14.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 33.6/113.7 MB 17.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 41.7/113.7 MB 19.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 52.2/113.7 MB 22.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 56.6/113.7 MB 23.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 59.2/113.7 MB 21.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 65.3/113.7 MB 22.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 74.2/113.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 81.5/113.7 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 83.6/113.7 MB 23.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 92.5/113.7 MB 24.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 101.7/113.7 MB 25.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 110.6/113.7 MB 26.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 113.7/113.7 MB 25.8 MB/s  0:00:04\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, safetensors, regex, pymupdf, pillow, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers\n",
      "\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   -- -------------------------------------  1/14 [scipy]\n",
      "   ----------- ----------------------------  4/14 [pymupdf]\n",
      "   ----------- ----------------------------  4/14 [pymupdf]\n",
      "   ----------- ----------------------------  4/14 [pymupdf]\n",
      "   -------------- -------------------------  5/14 [pillow]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   -------------------- -------------------  7/14 [MarkupSafe]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ---------------------------------------- 14/14 [sentence-transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 jinja2-3.1.6 joblib-1.5.3 networkx-3.4.2 pillow-12.1.0 pymupdf-1.26.7 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.7.2 scipy-1.15.3 sentence-transformers-5.2.2 threadpoolctl-3.6.0 torch-2.10.0 transformers-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pymupdf sentence-transformers chromadb pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f332444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x1df7e61dcc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459849e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\learning\\GEN-ai\\vectordb_VENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List , Dict , Any\n",
    "\n",
    "import fitz\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844d3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = r\"data\\1706.03762v7.pdf\"\n",
    "FIGURE_DIR = r\"data\\figure\"\n",
    "\n",
    "os.makedirs(FIGURE_DIR , exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730b812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.CloudClient(\n",
    "  api_key='ck-GPKiqKqGR2YzvTqw6kxYwAU1tccp5zLEmRGFBhkAc5Yf',\n",
    "  tenant='008c8583-0f4f-4a4e-ac8a-f40bfc731074',\n",
    "  database='imageandtext'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d345c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_COLLECTION_NAME = \"pdf_figure\"\n",
    "TEXT_COLLECTION_NAME = \"attention_text_chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e414933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "d:\\Work\\learning\\GEN-ai\\vectordb_VENV\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sagnik\\.cache\\huggingface\\hub\\models--sentence-transformers--clip-ViT-B-32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 872.91it/s, Materializing param=visual_projection.weight]                                \n",
      "CLIPModel LOAD REPORT from: C:\\Users\\Sagnik\\.cache\\huggingface\\hub\\models--sentence-transformers--clip-ViT-B-32\\snapshots\\327ab6726d33c0e22f920c83f2ff9e4bd38ca37f\\0_CLIPModel\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('clip-ViT-B-32') # dimention of model is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1263e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(path:str)->np.ndarray:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    emb = model.encode([img] , convert_to_numpy=True , show_progress_bar=False)\n",
    "    return emb[0]\n",
    "\n",
    "def encode_text(text:str)->np.ndarray:\n",
    "    \n",
    "    emb = model.encode([text] , convert_to_numpy=True , show_progress_bar=False)\n",
    "    return emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27035bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f14e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.25',\n",
       " 'creationDate': 'D:20240410211143Z',\n",
       " 'modDate': 'D:20240410211143Z',\n",
       " 'trapped': '',\n",
       " 'encryption': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579526ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image will crete one index and for text one index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c159ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images and captions...\n",
      "Page 1: found 0 images\n",
      "Page 2: found 0 images\n",
      "Page 3: found 1 images\n",
      "Page 4: found 2 images\n",
      "Page 5: found 0 images\n",
      "Page 6: found 0 images\n",
      "Page 7: found 0 images\n",
      "Page 8: found 0 images\n",
      "Page 9: found 0 images\n",
      "Page 10: found 0 images\n",
      "Page 11: found 0 images\n",
      "Page 12: found 0 images\n",
      "Page 13: found 0 images\n",
      "Page 14: found 0 images\n",
      "Page 15: found 0 images\n",
      "Total extracted images: 3\n"
     ]
    }
   ],
   "source": [
    "image_records: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"Extracting images and captions...\")\n",
    "for page_index, page in enumerate(doc, start=1):\n",
    "    blocks = page.get_text(\"blocks\")  # (x0, y0, x1, y1, text, ...)\n",
    "    text_blocks = [b for b in blocks if b[4].strip()]  # keep non-empty text blocks\n",
    "\n",
    "    images = page.get_images(full=True)\n",
    "    print(f\"Page {page_index}: found {len(images)} images\")\n",
    "\n",
    "    for img_index, img in enumerate(images, start=1):\n",
    "        xref = img[0]\n",
    "\n",
    "        # Where is this image on the page?\n",
    "        rects = page.get_image_rects(xref)\n",
    "        if not rects:\n",
    "            continue\n",
    "        rect = rects[0]\n",
    "\n",
    "        # Heuristic: caption = closest text block BELOW the image\n",
    "        below_blocks = [b for b in text_blocks if b[1] >= rect.y1]\n",
    "        candidates = below_blocks if below_blocks else text_blocks\n",
    "\n",
    "        if candidates:\n",
    "            closest_block = min(candidates, key=lambda b: abs(b[1] - rect.y1))\n",
    "            caption_text = closest_block[4].strip().replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            caption_text = \"\"\n",
    "\n",
    "        # Extract image binary and save to file\n",
    "        img_data = doc.extract_image(xref)\n",
    "        img_bytes = img_data[\"image\"]\n",
    "        img_ext = img_data.get(\"ext\", \"png\")\n",
    "\n",
    "        img_filename = f\"page_{page_index:02d}_img_{img_index}.{img_ext}\"\n",
    "        img_path = os.path.join(FIGURE_DIR, img_filename)\n",
    "\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "\n",
    "        image_records.append({\n",
    "            \"id\": f\"p{page_index}_img{img_index}\",\n",
    "            \"page_number\": page_index,\n",
    "            \"bbox_x0\": float(rect.x0),\n",
    "            \"bbox_y0\": float(rect.y0),\n",
    "            \"bbox_x1\": float(rect.x1),\n",
    "            \"bbox_y1\": float(rect.y1),\n",
    "            \"caption\": caption_text,\n",
    "            \"image_path\": img_path,\n",
    "        })\n",
    "\n",
    "print(f\"Total extracted images: {len(image_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1414432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
